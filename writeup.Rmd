---
title: "Practical Machine Learning"
author: "GHyee"
date: "Sunday, February 08, 2015"
output: html_document
---

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. In this project, the goal is to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants to predict how well they perform barbell lifts. More information is available from the website here: <http://groupware.les.inf.puc-rio.br/har> (see the section on the Weight Lifting Exercise Dataset). 

#Data Processing
The data source comes from <http://groupware.les.inf.puc-rio.br/har> and are already split into Train and Test set beforehand. 
The Train dataset can be downloaded from <https://d396qusza40orc.cloudfront.net/predmachlearn/pml-Train.csv> while the Test dataset can be downloaded from <https://d396qusza40orc.cloudfront.net/predmachlearn/pml-Test.csv>.
The data are processed after some exploratory analysis before models are fitted.
```{r, echo=TRUE}
#Load required library
library(caret)
library(rpart)
library(rpart.plot)
library(RColorBrewer)
library(rattle)
library(randomForest)
####################################
#############Load Data#############
####################################
#Download data from URL
#trainUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-Train.csv"
#testUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-Test.csv"
#Data have been downloaded to reduce runtime of the code
#Import data into R
Train <- read.csv("./pml-training.csv", na.strings=c("NA","#DIV/0!",""))
Test <- read.csv("./pml-testing.csv", na.strings=c("NA","#DIV/0!",""))

#Create partition on Train set
inTrain <- createDataPartition(y=Train$classe, p=0.6, list=FALSE)
myTrain <- Train[inTrain, ]; myTest <- Train[-inTrain, ]
dim(myTrain); dim(myTest)
#Remove variables with near zero variance                     
myDataNZV <- nearZeroVar(myTrain, saveMetrics=TRUE)

myNZVvars <- names(myTrain) %in% c("new_window", "kurtosis_roll_belt", "kurtosis_picth_belt",
                                      "kurtosis_yaw_belt", "skewness_roll_belt", "skewness_roll_belt.1", "skewness_yaw_belt",
                                      "max_yaw_belt", "min_yaw_belt", "amplitude_yaw_belt", "avg_roll_arm", "stddev_roll_arm",
                                      "var_roll_arm", "avg_pitch_arm", "stddev_pitch_arm", "var_pitch_arm", "avg_yaw_arm",
                                      "stddev_yaw_arm", "var_yaw_arm", "kurtosis_roll_arm", "kurtosis_picth_arm",
                                      "kurtosis_yaw_arm", "skewness_roll_arm", "skewness_pitch_arm", "skewness_yaw_arm",
                                      "max_roll_arm", "min_roll_arm", "min_pitch_arm", "amplitude_roll_arm", "amplitude_pitch_arm",
                                      "kurtosis_roll_dumbbell", "kurtosis_picth_dumbbell", "kurtosis_yaw_dumbbell", "skewness_roll_dumbbell",
                                      "skewness_pitch_dumbbell", "skewness_yaw_dumbbell", "max_yaw_dumbbell", "min_yaw_dumbbell",
                                      "amplitude_yaw_dumbbell", "kurtosis_roll_forearm", "kurtosis_picth_forearm", "kurtosis_yaw_forearm",
                                      "skewness_roll_forearm", "skewness_pitch_forearm", "skewness_yaw_forearm", "max_roll_forearm",
                                      "max_yaw_forearm", "min_roll_forearm", "min_yaw_forearm", "amplitude_roll_forearm",
                                      "amplitude_yaw_forearm", "avg_roll_forearm", "stddev_roll_forearm", "var_roll_forearm",
                                      "avg_pitch_forearm", "stddev_pitch_forearm", "var_pitch_forearm", "avg_yaw_forearm",
                                      "stddev_yaw_forearm", "var_yaw_forearm")
#Remove variables with near zero variance
myTrain <- myTrain[!myNZVvars]
#Check dimension of subset
dim(myTrain)
#Remove ID column (1)
myTrain <- myTrain[c(-1)]
#Remove variables with more than 60% of NAs
TrainTemp <- myTrain #creating another subset to iterate in loop
for(i in 1:length(myTrain)) { #for every column in the Train dataset
  if( sum( is.na( myTrain[, i] ) ) /nrow(myTrain) >= .6 ) { #if n?? NAs > 60% of total observations
    for(j in 1:length(TrainTemp)) {
      if( length( grep(names(myTrain[i]), names(TrainTemp)[j]) ) ==1)  { #if the columns are the same:
        TrainTemp <- TrainTemp[ , -j] #Remove that column
      }   
    } 
  }
}
#To check the new N?? of observations
dim(TrainTemp)

#Seting back to our set:
myTrain <- TrainTemp
rm(TrainTemp)
#Obtain variable names that are not removed earlier
clean1 <- colnames(myTrain)
clean2 <- colnames(myTrain[, -58]) #already with classe column removed
#Remove the variables from myTest set
myTest <- myTest[clean1]
Test <- Test[clean2]

#To check the new N?? of observations
dim(myTest)

#To check the new N?? of observations
dim(Test)
#Coerce Test set to match Train set
for (i in 1:length(Test) ) {
  for(j in 1:length(myTrain)) {
    if( length( grep(names(myTrain[i]), names(Test)[j]) ) ==1)  {
      class(Test[j]) <- class(myTrain[i])
    }      
  }      
}
#And to make sure Coertion really worked, simple smart ass technique:
Test <- rbind(myTrain[2, -58] , Test) #note row 2 does not mean anything, this will be removed right.. now:
Test <- Test[-1,]
#########################################################################
#######################End of Data Processing############################
#########################################################################
```

#Model Fitting
Two machine learning method, namely the decision tree and Random forest, will be used to fit the model. The method with the better accuracy will be chosen. Decision tree is fitted using the rpart function from the rpart library. Random forest is fitted using the randomForest function from the randomForest package.
```{r, echo=TRUE}
#Set set for reproducibility
set.seed(20151802)
modFitA1 <- rpart(classe ~ ., data=myTrain, method="class")

fancyRpartPlot(modFitA1)

predictionsA1 <- predict(modFitA1, myTest, type = "class")

confusionMatrix(predictionsA1, myTest$classe)

modFitB1 <- randomForest(classe ~. , data=myTrain)

predictionsB1 <- predict(modFitB1, myTest, type = "class")

confusionMatrix(predictionsB1, myTest$classe)

predictionsB2 <- predict(modFitB1, Test, type = "class")
```

The second model, Random Forest, will be chosen as it yields a much better accuracy compared to the first. 

The test data set is used to validate the accuracy of the Random forest model.
```{r, echo=TRUE}
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}
pml_write_files(predictionsB2)
```
